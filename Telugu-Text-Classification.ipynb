{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Telugu Text Classification\n",
    "\n",
    "This notebook demonstrates a text classification pipeline for Telugu text using TensorFlow/Keras and Hugging Face Transformers. The dataset appears to be categorized into labels such as \"positive\" and \"negative\" or similar categories.\n",
    "\n",
    "Steps included:\n",
    "1. Data loading and preprocessing\n",
    "2. Tokenization using Hugging Face tokenizer\n",
    "3. Creating TensorFlow datasets\n",
    "4. Model creation with Transformers + Keras\n",
    "5. Training with callbacks and saving the model\n",
    "\n",
    "Note: Paths and dataset specifics may need to be adjusted depending on your environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "from transformers import AutoTokenizer, TFAutoModel\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "print(\"Libraries imported\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seed for reproducibility\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "\n",
    "print(\"Seed set\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset (update path as needed)\n",
    "data_path = \"telugu_dataset.csv\"\n",
    "\n",
    "if os.path.exists(data_path):\n",
    "    df = pd.read_csv(data_path)\n",
    "else:\n",
    "    # Example dataframe structure if file not present\n",
    "    df = pd.DataFrame({\n",
    "        'text': [\"నేను సంతోషంగా ఉన్నాను\", \"నాకు బాధగా ఉంది\", \"ఇది అద్భుతమైన వనరు\"],\n",
    "        'label': [\"positive\", \"negative\", \"positive\"]\n",
    "    })\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode labels\n",
    "label2id = {label: idx for idx, label in enumerate(df['label'].unique())}\n",
    "id2label = {v: k for k, v in label2id.items()}\n",
    "\n",
    "df['label_id'] = df['label'].map(label2id)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-test split\n",
    "train_df, val_df = train_test_split(df, test_size=0.2, random_state=SEED, stratify=df['label_id'])\n",
    "\n",
    "print(f\"Train size: {len(train_df)}, Val size: {len(val_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizer and model selection\n",
    "MODEL_NAME = \"xlm-roberta-base\"  # or a Telugu-specific model if available\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "base_model = TFAutoModel.from_pretrained(MODEL_NAME)\n",
    "\n",
    "MAX_LEN = 128\n",
    "\n",
    "print(\"Tokenizer and model loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_texts(texts, tokenizer, max_len=128):\n",
    "    encodings = tokenizer(texts.tolist(), truncation=True, padding='max_length', max_length=max_len)\n",
    "    return np.array(encodings['input_ids']), np.array(encodings['attention_mask'])\n",
    "\n",
    "train_input_ids, train_attention_mask = encode_texts(train_df['text'], tokenizer, MAX_LEN)\n",
    "val_input_ids, val_attention_mask = encode_texts(val_df['text'], tokenizer, MAX_LEN)\n",
    "\n",
    "train_labels = train_df['label_id'].values\n",
    "val_labels = val_df['label_id'].values\n",
    "\n",
    "print(\"Data encoded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 8\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices(((train_input_ids, train_attention_mask), train_labels))\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices(((val_input_ids, val_attention_mask), val_labels))\n",
    "\n",
    "train_dataset = train_dataset.shuffle(100).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "val_dataset = val_dataset.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "print(\"Datasets created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, Dropout, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "def create_model(base_model, max_len=128, num_labels=2):\n",
    "    input_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"input_ids\")\n",
    "    attention_mask = Input(shape=(max_len,), dtype=tf.int32, name=\"attention_mask\")\n",
    "    \n",
    "    outputs = base_model(input_ids, attention_mask=attention_mask)\n",
    "    pooled_output = outputs[0][:, 0, :]  # take <s> token (or [CLS] equivalent)\n",
    "    x = Dropout(0.3)(pooled_output)\n",
    "    x = Dense(128, activation=\"relu\")(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    logits = Dense(num_labels, activation=\"softmax\")(x)\n",
    "    \n",
    "    model = Model(inputs=[input_ids, attention_mask], outputs=logits)\n",
    "    return model\n",
    "\n",
    "model = create_model(base_model, MAX_LEN, num_labels=len(label2id))\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=2e-5),\n",
    "              loss=\"sparse_categorical_crossentropy\",\n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = \"telugu_text_classifier.h5\"\n",
    "checkpoint = ModelCheckpoint(checkpoint_path, monitor='val_accuracy', save_best_only=True, save_weights_only=True, verbose=1)\n",
    "earlystopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "\n",
    "history = model.fit(train_dataset,\n",
    "                    validation_data=val_dataset,\n",
    "                    epochs=3,\n",
    "                    callbacks=[checkpoint, earlystopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"telugu_text_classifier_full\")\n",
    "print(\"Model saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference example\n",
    "\n",
    "Use the tokenizer and the saved model for inference. Remember to load weights and ensure tokenization settings match training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(texts, tokenizer, model, max_len=128):\n",
    "    input_ids, attention_mask = encode_texts(pd.Series(texts), tokenizer, max_len)\n",
    "    preds = model.predict([input_ids, attention_mask])\n",
    "    return [id2label[np.argmax(p)] for p in preds]\n",
    "\n",
    "# Example\n",
    "sample_texts = [\"నేను చాలా సంతోషంగా ఉన్నాను\", \"ఈ విషయం నన్ను బాధపెడుతుంది\"]\n",
    "print(predict(sample_texts, tokenizer, model, MAX_LEN))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
